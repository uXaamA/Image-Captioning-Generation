# Image Captioning with CNN + LSTM

## ğŸ“Œ Project Overview

This project implements an **Image Captioning Generation System** using the **Flickr8K** dataset. The model leverages a CNN (ResNet-50) for image encoding and an LSTM network (with attention) for decoding into natural language descriptions.

It is part of a Deep Learning assignment focused on learning and building multi-modal deep learning systems that combine vision and language.

---

## ğŸ‘¤ Author

**Muhammad Usama**
MSc Data Science 
Technical Data Anlayst at Haier Pakistan | Aspiring Data Scientist & Entrepreneur

---

## ğŸ“‚ Dataset

* **Name**: Flickr8K
* **Total Images**: 8000
* **Captions per Image**: 5
* **Train/Val/Test Split**:

  * Training: 7000 images
  * Validation: 1000 images
  * Testing: 200 images
* **Vocabulary Size**: 8,765 unique words
* **Max Caption Length**: 35 tokens

---

## ğŸ§  Methodology

### ğŸ”§ Preprocessing

**Image Preprocessing:**

* Resize to 224x224
* Normalize using:

  * `mean = [0.485, 0.456, 0.406]`
  * `std  = [0.229, 0.224, 0.225]`

**Text Preprocessing:**

* Lowercase all captions
* Add `<sos>` and `<eos>` tokens
* Remove special characters
* Tokenize words and build vocabulary

### ğŸ” Vocabulary

* Unique words mapped to integer IDs
* Embeddings generated using `nn.Embedding`
* Handled rare words using `<unk>` token

### ğŸ—ï¸ Model Architecture

**Image Encoder:**

* `ResNet-50` pretrained on ImageNet
* Removed final classification layer

**Text Decoder:**

* 2-layer LSTM with attention
* Concatenated image and word embeddings
* Generated sequence of words (caption)

### âš™ï¸ Training Setup

| Parameter     | Value        |
| ------------- | ------------ |
| Epochs        | 100          |
| Batch Size    | 128          |
| Learning Rate | 5e-4         |
| Optimizer     | AdamW        |
| Loss          | CrossEntropy |

**Key Techniques:**

* Mixed Precision Training
* Learning Rate Scheduling
* Early Stopping (patience = 5)
* Gradient Scaling

---

## ğŸ“ˆ Results

### ğŸ”¬ Performance

| Metric | Training | Validation |
| ------ | -------- | ---------- |
| Loss   | 1.24     | 2.87       |
| BLEU-1 | 0.62     | 0.54       |
| BLEU-4 | 0.31     | 0.23       |

### ğŸ§  Sample Predictions

**Example 1:**

* Actual: *"A group of people playing volleyball on sandy beach"*
* Predicted: *"People are playing game on beach with net"*

**Example 2:**

* Actual: *"Busy city street with yellow taxis and pedestrians"*
* Predicted: *"Urban road with cars and people walking"*

---

## ğŸ”¬ Hyperparameter Analysis

| Learning Rate | Val Loss | Epochs |
| ------------- | -------- | ------ |
| 1e-3          | 3.45     | 28     |
| 5e-4          | 2.87     | 35     |
| 1e-4          | 3.12     | 52     |

---

## ğŸš§ Challenges & Solutions

* **Overfitting** â†’ Applied dropout (0.5) and early stopping
* **Training time** â†’ Used mixed precision (40% speedup)
* **Rare words** â†’ Added `<unk>` token for words with <5 frequency

---

## ğŸ¯ Conclusion

* Successfully achieved **54% BLEU-1** score on validation
* Model captures core semantics of image content
* Attention helps in focusing on relevant visual regions

### ğŸš€ Future Improvements

* Use larger datasets like Flickr30K
* Experiment with Transformer-based architectures
* Add Beam Search decoding for better caption fluency

---

## ğŸ§ª Execution Instructions

```bash
# 1. Install dependencies
pip install -r requirements.txt
wandb login

# 2. Train the model
python main.py \
  --dataset_path /path/to/dataset \
  --mode train \
  --wandb_entity YOUR_WANDB_USERNAME

# 3. Test the model on custom image
python main.py \
  --mode test \
  --image_path /path/to/image.jpg \
  --wandb_entity YOUR_WANDB_USERNAME
```

---

## ğŸ’¡ Key Features

* ğŸ” Mixed precision training (faster training)
* ğŸ“ˆ Full Weights & Biases logging support
* âš™ï¸ Multi-GPU support using `DataParallel`
* ğŸ“‰ LR scheduler, early stopping, ReduceLROnPlateau
* ğŸ“Š Graphs: Loss vs Epoch, BLEU vs Epoch
* ğŸ–¼ï¸ Caption visualization over test images

---

## ğŸ“ Project Structure

```
project/
â”œâ”€â”€ main.py
â”œâ”€â”€ train.py
â”œâ”€â”€ test.py
â”œâ”€â”€ test_eval.py
â”œâ”€â”€ model.py
â”œâ”€â”€ data_utils.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ graphs/
â”œâ”€â”€ weights/            # Checkpoint weights
â”œâ”€â”€ Report.pdf          # Analysis and explanation
â””â”€â”€ README.md           # You are here âœ¨
```

---

Feel free to contribute or extend the project with new ideas! ğŸš€
